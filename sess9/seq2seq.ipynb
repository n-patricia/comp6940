{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEp376Zv8zd9"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWiw7-7lIH8O",
        "outputId": "82c432c6-9763-43d9-addb-6b968e90548f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 13:54:05--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 108.138.94.103, 108.138.94.65, 108.138.94.32, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|108.138.94.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-29 13:54:05 (65.8 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "--2024-04-29 13:54:05--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2024-04-29 13:54:05--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 2.7M in 0.04s (65.8 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./datasets"
      ],
      "metadata": {
        "id": "Y3pYXAJsIH2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip -d ./datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41dCrIYfIHy4",
        "outputId": "d7a5fc83-cb1a-4036-d262-81d6ae833e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: ./datasets/data/\n",
            "  inflating: ./datasets/data/eng-fra.txt  \n",
            "   creating: ./datasets/data/names/\n",
            "  inflating: ./datasets/data/names/Arabic.txt  \n",
            "  inflating: ./datasets/data/names/Chinese.txt  \n",
            "  inflating: ./datasets/data/names/Czech.txt  \n",
            "  inflating: ./datasets/data/names/Dutch.txt  \n",
            "  inflating: ./datasets/data/names/English.txt  \n",
            "  inflating: ./datasets/data/names/French.txt  \n",
            "  inflating: ./datasets/data/names/German.txt  \n",
            "  inflating: ./datasets/data/names/Greek.txt  \n",
            "  inflating: ./datasets/data/names/Irish.txt  \n",
            "  inflating: ./datasets/data/names/Italian.txt  \n",
            "  inflating: ./datasets/data/names/Japanese.txt  \n",
            "  inflating: ./datasets/data/names/Korean.txt  \n",
            "  inflating: ./datasets/data/names/Polish.txt  \n",
            "  inflating: ./datasets/data/names/Portuguese.txt  \n",
            "  inflating: ./datasets/data/names/Russian.txt  \n",
            "  inflating: ./datasets/data/names/Scottish.txt  \n",
            "  inflating: ./datasets/data/names/Spanish.txt  \n",
            "  inflating: ./datasets/data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "metadata": {
        "id": "DOhiU-qNIHwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"./datasets/data\""
      ],
      "metadata": {
        "id": "u22AbXn1UOAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "    self.n_words = 2 # count SOS and EOS\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "l0jiQ7pvJ42y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn'\n",
        "  )"
      ],
      "metadata": {
        "id": "81A0K42RQcAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "  return s.strip()"
      ],
      "metadata": {
        "id": "c5izgjs1Qb9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(data_dir, lang1, lang2, reverse=False):\n",
        "  print(\"Reading lines...\")\n",
        "\n",
        "  # Read the file and split into lines\n",
        "  lines = open(f'{data_dir}/{lang1}-{lang2}.txt', encoding='utf-8').\\\n",
        "    read().strip().split('\\n')\n",
        "\n",
        "  # Split every line into pairs and normalize\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "  # Reverse pairs, make Lang instances\n",
        "  if reverse:\n",
        "    pairs = [list(reversed(p)) for p in pairs]\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "  else:\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "PrP1QfKwQj3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "  \"i am \", \"i m \",\n",
        "  \"he is\", \"he s \",\n",
        "  \"she is\", \"she s \",\n",
        "  \"you are\", \"you re \",\n",
        "  \"we are\", \"we re \",\n",
        "  \"they are\", \"they re \"\n",
        ")"
      ],
      "metadata": {
        "id": "Ws77EzZZQj0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "    len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "    p[1].startswith(eng_prefixes)"
      ],
      "metadata": {
        "id": "DDQfZtj7Qjx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "Qtf8vH9cJ4zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proses of preparing data:\n",
        "\n",
        "\n",
        "*   Read text file and split into lines,\n",
        "*   Split lines into pairs\n",
        "*   Normalize text, filter by length and content\n",
        "*   Make word lists from sentences in pairs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vHs4_OZeSeEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(data_dir, lang1, lang2, reverse=False):\n",
        "  input_lang, output_lang, pairs = readLangs(data_dir, lang1, lang2, reverse)\n",
        "  print(\"Read %s sentence pairs\" % len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "      input_lang.addSentence(pair[0])\n",
        "      output_lang.addSentence(pair[1])\n",
        "  print(\"Counted words:\")\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "xGKF01qyJ4mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData(data_dir, 'eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsxN3vV3IHtH",
        "outputId": "dc952893-5e42-4658-d511-00cf2ccdcc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11445 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "['je ne suis pas un menteur', 'i m not a liar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "691YqmSzTtes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Wj18ieXTtbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4SG2ta0TtX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rehEfD-FIHp_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}